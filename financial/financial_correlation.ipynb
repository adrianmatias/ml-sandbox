{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce2ae8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f3b874",
   "metadata": {},
   "source": [
    "# Financial correlation for FED impact in macro\n",
    "\n",
    "This notebook explores the impact of the FED policies on macroeconomic variables.\n",
    "It describes the relations by the correlations between their historical values.\n",
    "\n",
    "### FED controlled series:\n",
    "- Federal Funds Effective Rate\n",
    "- 10Y-2Y Treasury Constant Maturity\n",
    "- Balance Sheet: Total Assets\n",
    "### macroeconomic variables, for United States:\n",
    "- Crude Oil Price: WTI\n",
    "- Sticky Price Consumer Price Index less Food and Energy\n",
    "- Velocity of M2 Money Stock\n",
    "- Unemployment Rate\n",
    "- GDP\n",
    "- gold\n",
    "- SP500\n",
    "\n",
    "## Data\n",
    "- 1970 - 2023\n",
    "- main source https://fred.stlouisfed.org/\n",
    "- gold source\n",
    "    - https://datahub.io/AcckiyGerman/gold-prices#data\n",
    "    - https://www.nasdaq.com/market-activity/commodities/gc:cmx/historical\n",
    "- sp500 source\n",
    "    - https://www.investing.com/indices/us-spx-500-historical-data\n",
    "\n",
    "## Approach\n",
    "- unify timeframe and aggregate into monthly resolution\n",
    "- merge data\n",
    "- build percent variation and shifted variables\n",
    "- rolling scale series to z-score (mean 0, standard deviation 1)\n",
    "- alculate pairwise correlation coefficients, as the median of the rolling correlation across time for every pair\n",
    "- plot the most relevant and correlated pairs of series\n",
    "- add the rolling correlation across time to each pair plot\n",
    "\n",
    "## Results\n",
    "- high-interest rates are related to:\n",
    "    - high GDP 6 months before\n",
    "    - low unemployment rate\n",
    "    - high m2 money velocity, strong correlation after early 2000â€™s\n",
    "    - high CPI 12 months after\n",
    "- high treasury bond inversion 10-2 years is related to:\n",
    "    - high unemployment rate, strong correlation\n",
    "    - high GDP 6 months before\n",
    "    - low GDP rate y/y 12 months before\n",
    "- high balance sheet is related to:\n",
    "    - high GDP 6 months before\n",
    "    - low unemployment 6 months before\n",
    "- high balance sheet rate y/y is related to:\n",
    "    - low GDP rate y/y\n",
    "    - low unemployment rate 6 months after\n",
    "    \n",
    "- balance sheet correlations invert after 2019Q4, related to its sudden increase during the event of September in the repo market https://en.wikipedia.org/wiki/September_2019_events_in_the_U.S._repo_market\n",
    "- no strong correlation between FED policies and oil, gold price\n",
    "\n",
    "## Notes\n",
    "- Series shifted a positive number of months reflect delayed sequences, which correspond with a number of periods in the past relative to the fixed series. Moving a GDP +6 months means bringing to the present data from the past.\n",
    "- Spearman and Kendall-Tau methods for calculating correlation coefficients are not implemented as rolling correlation in pandas. It would be worthy to check them to be more robust against non-linearity in order to evaluate whether variables are at least monotonically correlated.\n",
    "- SP500 has been analyzed by taking its logarithm to alleviate the exponential distribution, with no remarkable differences.\n",
    "\n",
    "## Insights\n",
    "- Go back to develop code and explore data to PyCharm while keeping the notebook just to display notes, results, plots, and high-level code.\n",
    "- When working with time dependant data apply any transformation in a rolling manner, even if is not inferential or simply descriptive. This applies to scaling, correlation, handling missing data...\n",
    "\n",
    "## Next\n",
    "- Research oil and gold data in a smaller time resolution scale. Since their sizes are smaller than GDP or balance sheet, they move in shorter time frames.\n",
    "- Analyze relationships using supervised machine learning instead of correlation. This would involve taking Fed policy series as x and each impacted variable as y. Feature importance and partial dependency plots might produce insight variable relationships. The accuracy of the model for each target series can select those predictable by the input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "251b3f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pd\n",
    "import numpy as np\n",
    "\n",
    "import datapackage\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af6ef282",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_date = \"DATE\"\n",
    "col_month = \"month\"\n",
    "\n",
    "col_gold = \"gold\"\n",
    "col_interest_rate = \"Federal Funds Effective Rate\"\n",
    "col_yield_spread = \"10Y-2Y Treasury Constant Maturity\"\n",
    "col_balance_sheet = \"Balance Sheet: Total Assets\"\n",
    "col_oil = \"Crude Oil Price: WTI\"\n",
    "col_cpi = \"Sticky Price Consumer Price Index less Food and Energy\"\n",
    "col_m2v = \"Velocity of M2 Money Stock\"\n",
    "col_unemployment = \"Unemployment Rate\"\n",
    "col_gdp = \"GDP\"\n",
    "col_gold = \"gold\"\n",
    "col_sp500 = \"SP500\"\n",
    "\n",
    "alias_dict = {\n",
    "    \"RIFSPFFNB\": col_interest_rate,\n",
    "    \"T10Y2Y\": col_yield_spread,\n",
    "    \"QBPBSTAS\": col_balance_sheet,\n",
    "    \"DCOILWTICO\": col_oil,\n",
    "    \"CORESTICKM159SFRBATL\": col_cpi,\n",
    "    \"M2V\": col_m2v,\n",
    "    \"UNRATE\": col_unemployment,\n",
    "}\n",
    "\n",
    "suffix_pct_chg = \"_percent_change_y/y\"\n",
    "\n",
    "col_policy_list = [\n",
    "    col_interest_rate,\n",
    "    col_yield_spread,\n",
    "    col_balance_sheet,\n",
    "    col_balance_sheet + suffix_pct_chg,\n",
    "]\n",
    "\n",
    "year_start = 1970\n",
    "year_end = 2023\n",
    "rolling_window_month_count = 12 * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1b8ac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month_start(series: Series) -> Series:\n",
    "    return series.values.astype(\"<M8[M]\")\n",
    "\n",
    "def agg_monthly(df: DataFrame) -> DataFrame:\n",
    "    df[col_date] = get_month_start(df[col_date])\n",
    "    # return df.set_index(col_date).resample(\"MS\").median()\n",
    "    return df.groupby(col_date, as_index=False).median()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5daccf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_gold_new():\n",
    "    from datetime import datetime\n",
    "    dateparse = lambda x: datetime.strptime(x, '%m/%d/%Y')\n",
    "\n",
    "    df = pd.read_csv(\"data/gold_2023-02-22.csv\", parse_dates=['Date'], date_parser=dateparse)\n",
    "    df.columns = [col.upper() for col in df.columns]\n",
    "    col_price = \"CLOSE/LAST\"\n",
    "    col_gold_new = col_gold + \"_new\"\n",
    "\n",
    "    return df[[col_date, col_price]].rename(columns={col_price: col_gold_new})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abd1b64f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_df_gold_old():\n",
    "\n",
    "    col_gold_old = col_gold + \"_old\"\n",
    "    df = pd.read_csv(\"data/gold.csv\", try_parse_dates=True)\n",
    "\n",
    "    df.columns = [col.upper() for col in df.columns]\n",
    "\n",
    "    return df.rename(columns={\"PRICE\": col_gold_old})[[col_date, col_gold_old]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74532e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_gold():\n",
    "    df = pd.merge(\n",
    "        left=agg_monthly(get_df_gold_old()),\n",
    "        right=agg_monthly(get_df_gold_new()),\n",
    "        how=\"outer\",\n",
    "    )\n",
    "    df[col_gold] = df[col_gold + \"_old\"].combine_first(df[col_gold + \"_new\"])\n",
    "    return df[[col_date, col_gold]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c719bf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_sp500():\n",
    "    df = pd.read_csv(\"data/S&P 500 Historical Data.csv\", try_parse_dates=True, thousands=\",\")\n",
    "\n",
    "    df.columns = [col.upper() for col in df.columns]\n",
    "    col = \"SP500\"\n",
    "    df = df.rename(columns={\"PRICE\": col})[[col_date, col]]\n",
    "    # df[col] = np.log(df[col])\n",
    "    return agg_monthly(df.bfill())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dc2ac56",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'polars' has no attribute 'merge'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 16\u001b[0m\n\u001b[1;32m      1\u001b[0m df_list \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     pd\u001b[38;5;241m.\u001b[39mdate_range(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear_start\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear_end\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1mo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m ] \u001b[38;5;241m+\u001b[39m [\n\u001b[1;32m      4\u001b[0m     pd\u001b[38;5;241m.\u001b[39mread_csv(file_name, try_parse_dates\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, null_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/GDP.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/DCOILWTICO.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/RIFSPFFNB.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/T10Y2Y.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/CORESTICKM159SFRBATL.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/M2V.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/UNRATE.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/QBPBSTAS.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;66;03m# \"data/IQ12260.csv\",\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     ]\n\u001b[1;32m     15\u001b[0m ] \u001b[38;5;241m+\u001b[39m [\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mget_df_gold\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     17\u001b[0m     get_df_sp500(),\n\u001b[1;32m     18\u001b[0m ]\n",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m, in \u001b[0;36mget_df_gold\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_df_gold\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m(\n\u001b[1;32m      3\u001b[0m         left\u001b[38;5;241m=\u001b[39magg_monthly(get_df_gold_old()),\n\u001b[1;32m      4\u001b[0m         right\u001b[38;5;241m=\u001b[39magg_monthly(get_df_gold_new()),\n\u001b[1;32m      5\u001b[0m         how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     )\n\u001b[1;32m      7\u001b[0m     df[col_gold] \u001b[38;5;241m=\u001b[39m df[col_gold \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_old\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcombine_first(df[col_gold \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_new\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df[[col_date, col_gold]]\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'polars' has no attribute 'merge'"
     ]
    }
   ],
   "source": [
    "df_list = [\n",
    "    pd.date_range(f\"{year_start}\",f\"{year_end}\", interval=\"1mo\")\n",
    "] + [\n",
    "    pd.read_csv(file_name, try_parse_dates=True, null_values=\".\") for file_name in [\n",
    "        \"data/GDP.csv\",\n",
    "        \"data/DCOILWTICO.csv\",\n",
    "        \"data/RIFSPFFNB.csv\",\n",
    "        \"data/T10Y2Y.csv\",\n",
    "        \"data/CORESTICKM159SFRBATL.csv\",\n",
    "        \"data/M2V.csv\",\n",
    "        \"data/UNRATE.csv\",\n",
    "        \"data/QBPBSTAS.csv\",\n",
    "        # \"data/IQ12260.csv\",\n",
    "    ]\n",
    "] + [\n",
    "    get_df_gold(),\n",
    "    get_df_sp500(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379fb0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_df_list(df_list):\n",
    "    df = reduce(\n",
    "        lambda left,right: pd.merge(left=left, right=right, how=\"left\"),\n",
    "        map(agg_monthly, df_list)\n",
    "    )\n",
    "    df = df.set_index(col_date).sort_index()\n",
    "    df = df[df.index >= f\"{year_start}\"]\n",
    "    df.columns = [alias_dict.get(col, col) for col in df.columns]\n",
    "    \n",
    "    return df.bfill()\n",
    "\n",
    "df = combine_df_list(df_list=df_list)\n",
    "df_col_list = df.columns\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cdd015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_percent_change(df: DataFrame, col: str) -> DataFrame:\n",
    "    \n",
    "    freq_month_start = \"MS\"\n",
    "    s = (\n",
    "        df[col]\n",
    "        .resample(freq_month_start)\n",
    "        .median()\n",
    "    ) + 1\n",
    "    \n",
    "    df[col + suffix_pct_chg] = s.pct_change(periods=12)\n",
    "    return df\n",
    "\n",
    "col_not_rate_list = [\n",
    "    col_gdp,\n",
    "    col_gold,\n",
    "    col_balance_sheet,\n",
    "    col_m2v,\n",
    "    col_oil,\n",
    "    col_sp500,\n",
    "]\n",
    "\n",
    "for col in col_not_rate_list:\n",
    "    add_percent_change(df, col=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53df0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    df[col].plot()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77161117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_shifted(df: DataFrame, col: str, period_count: int) -> DataFrame:\n",
    "    df = df.copy()\n",
    "    df[f\"{col}{period_count:+d}_months\"] = df[col].shift(periods=period_count)\n",
    "    return df\n",
    "\n",
    "for period_count in [-24, -18, -12, -6, 6, 12, 18, 24]:\n",
    "    for col in [\n",
    "        col_gdp,\n",
    "        col_oil,\n",
    "        col_sp500,\n",
    "        col_unemployment,\n",
    "        col_cpi,\n",
    "        col_gold,\n",
    "        col_gdp + suffix_pct_chg,\n",
    "        col_oil + suffix_pct_chg,\n",
    "        col_gold + suffix_pct_chg,\n",
    "        col_sp500 + suffix_pct_chg,\n",
    "    ]:\n",
    "        df = add_shifted(df=df, col=col, period_count=period_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06068cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soften_series(df):\n",
    "    return df.rolling(\"100D\").median()\n",
    "# df = soften_series(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de75d0-5292-4002-ac64-ba2a63a8c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean = df.rolling(window=rolling_window_month_count).mean()\n",
    "df_std = df.rolling(window=rolling_window_month_count).std()\n",
    "\n",
    "df_scaled = (df - df_mean) / df_std\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd03dfe8-cc87-4ede-a8ba-1f1f13cd70be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = (\n",
    "    df_scaled\n",
    "    .rolling(rolling_window_month_count)\n",
    "    .corr()\n",
    "    [col_policy_list].\n",
    "    reset_index()\n",
    "    .groupby(\"level_1\")\n",
    "    .median()\n",
    ")\n",
    "df_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec62f8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_top_corr(df: DataFrame, col: str) -> DataFrame:\n",
    "    df = df.copy()\n",
    "    col_abs = col + \"_abs\"\n",
    "    df[col_abs] = df[col].abs()\n",
    "    return df.sort_values(col_abs)[col].tail(20)\n",
    "\n",
    "for col in col_policy_list:\n",
    "    get_top_corr(df=df_corr, col=col).plot.barh()\n",
    "    plt.title(f\"correlation: {col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65505086",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr_pair_list = [\n",
    "    [col_interest_rate, col_yield_spread],\n",
    "    [col_interest_rate, col_gdp + \"+6_months\"],\n",
    "    [col_interest_rate, col_unemployment],\n",
    "    [col_interest_rate, col_m2v],\n",
    "    [col_interest_rate, col_cpi + \"-12_months\"],\n",
    "    \n",
    "    [col_yield_spread, col_unemployment],\n",
    "    [col_yield_spread, col_gdp + suffix_pct_chg + \"+12_months\"],\n",
    "\n",
    "    [col_balance_sheet, col_gdp + \"+6_months\"],\n",
    "    [col_balance_sheet, col_unemployment + \"+6_months\"],\n",
    "\n",
    "    [col_balance_sheet + suffix_pct_chg, col_gdp],\n",
    "    [col_balance_sheet + suffix_pct_chg, col_unemployment + \"-6_months\"],\n",
    "    [col_balance_sheet + suffix_pct_chg, col_sp500],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02738b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_a, col_b in corr_pair_list:\n",
    "    \n",
    "    figsize = (10,8)\n",
    "    \n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=2,\n",
    "        ncols=1,\n",
    "        sharex=True\n",
    "    )\n",
    "\n",
    "    ax_a = df_scaled[col_a].plot(label=col_a, figsize=figsize, ax=axes[0])\n",
    "\n",
    "    ax_b = df_scaled[col_b].plot(label=col_b, secondary_y=True, ax=axes[0])\n",
    "\n",
    "    h1, l1 = ax_a.get_legend_handles_labels()\n",
    "    h2, l2 = ax_b.get_legend_handles_labels()\n",
    "\n",
    "    plt.legend(h1+h2, l1+l2, loc=2)\n",
    "    \n",
    "    ax_c = (\n",
    "        df_scaled[col_a]\n",
    "        .rolling(rolling_window_month_count)\n",
    "        .corr(df_scaled[col_b])\n",
    "        .plot\n",
    "        .area(\n",
    "            label=f\"rolling_correlation_{rolling_window_month_count}_months\", \n",
    "            stacked=False,\n",
    "            ax=axes[1]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    ax_c.legend(loc=3)\n",
    "    \n",
    "    corr_coef = df_corr.loc[col_b][col_a]\n",
    "    plt.title(f\"corr coef: {corr_coef}\")\n",
    "    plt.suptitle(f\"{col_a}, {col_b}\\n(rolling scaled)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
