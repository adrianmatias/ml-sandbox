{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Train BigramLanguageModel\n",
        "\n",
        "    This notebook trains a neural network based on the lecture\n",
        "    https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
        "    Let's build GPT: from scratch, in code, spelled out. by Andrej Karpathy.\n",
        "\n",
        "    The objective is to assimilate the contents of the lecture and practice with pytorch, transformer architecture, and neural network training.\n",
        "\n",
        "    This is approached by implementing a gpt-like model from scratch using the layers pytorch provides, instead of the custom ones from the lecture, to replicate the results of loss and text generation."
      ],
      "metadata": {
        "id": "gEOlp-zwr7xt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "bShcct73hSc0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from bigram_language_model_karpathy import BigramLanguageModelKarpathy\n",
        "from bigram_language_model_torch_layers import BigramLanguageModelTorchLayers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "p7dDRd6Qiddm",
        "outputId": "9f3e46d9-eb0e-4cba-a965-86e59dc4cd17"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLKvE1qHsLY4",
        "outputId": "a56242f0-7d26-4d35-b5b1-150f2007ed2a"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-01 14:54:01--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.10’\n",
            "\n",
            "\rinput.txt.10          0%[                    ]       0  --.-KB/s               \rinput.txt.10        100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-03-01 14:54:01 (30.2 MB/s) - ‘input.txt.10’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "block_size = 128\n",
        "max_iters = 1000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "dropout = 0.2\n",
        "eval_iters = 200\n",
        "number_layers = 2\n",
        "number_heads = 4\n",
        "number_embeddings = number_heads * 32  # 384 / 6 = 64 dimensional heads\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    corpus = f.read()\n",
        "\n",
        "# The properties of the test\n",
        "chars = sorted(list(set(corpus)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Encoding and decoding\n",
        "string_to_int = {character: index for index, character in enumerate(chars)}\n",
        "int_to_string = {index: character for index, character in enumerate(chars)}\n",
        "encode = lambda string: [string_to_int[char] for char in string]\n",
        "decode = lambda list_int: \"\".join([int_to_string[integer] for integer in list_int])\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(corpus), dtype=torch.long, device=device)\n",
        "number_train = int(0.9 * len(data))\n",
        "train_data = data[:number_train]\n",
        "validation_data = data[:number_train]"
      ],
      "metadata": {
        "id": "0q357f0xmIdc"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "is_model_karpathy = False\n",
        "\n",
        "if is_model_karpathy:\n",
        "    model = BigramLanguageModelKarpathy(\n",
        "        vocab_size=vocab_size,\n",
        "        number_embeddings=number_embeddings,\n",
        "        block_size=block_size,\n",
        "        number_heads=number_heads,\n",
        "        number_layers=number_layers,\n",
        "        dropout=dropout,\n",
        "        device=device,\n",
        "    ).to(device)\n",
        "else:\n",
        "    model = BigramLanguageModelTorchLayers(\n",
        "        vocab_size=vocab_size,\n",
        "        number_embeddings=number_embeddings,\n",
        "        block_size=block_size,\n",
        "        number_heads=number_heads,\n",
        "        number_layers=number_layers,\n",
        "        dropout=dropout,\n",
        "        device=device,\n",
        "    ).to(device)"
      ],
      "metadata": {
        "id": "OpV3td7Irxez"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "model.train_model(\n",
        "    max_iters,\n",
        "    train_data,\n",
        "    validation_data,\n",
        "    optimizer,\n",
        "    batch_size,\n",
        "    eval_interval,\n",
        "    eval_iters,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1m_MKG4igeA",
        "outputId": "9ee80f62-7d26-46b9-be03-a891308120eb"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At step 0: train loss 4.2914, val loss 4.2902.\n",
            "At step 500: train loss 2.4619, val loss 2.4626.\n",
            "At step 999: train loss 2.3506, val loss 2.3495.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate from the model\n",
        "print(\"\\nAn example of text generated from the model.\")\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, 300)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oypybsRxtZy_",
        "outputId": "a31da92d-3121-42a4-8819-26c6e2cb3531"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "An example of text generated from the model.\n",
            "\n",
            "CENI:\n",
            "Whofowe th sarod, theJor. mplay thereis re.\n",
            "Y RUK:\n",
            "\n",
            "DI pr Pumer d yovaroubomsed t,hy dll, adirea d the. Ris Ve?\n",
            "SQ\n",
            "\n",
            "ORESABOMALARUCERIH:\n",
            "Whore Whyathe ' VETofre S:\n",
            "Are chas ckdsor\n",
            "Ye wndes't may lowe tod, t, ssswin opamspe\n",
            "The llleaits pater los h o s langorabe hyot ng\n",
            "An wice h and: habeld onk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nNow we generate text forever.\\n\")\n",
        "model.generate_forever(decode, 0.1)"
      ],
      "metadata": {
        "id": "lnMYhRrerCcW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b4f6d35-bac6-480f-cd41-8acb8091076e"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Now we generate text forever.\n",
            "\n",
            "\n",
            "\n",
            "Sititee rd th woutoucrs. PUThy she, thy whor maw---pe F icoureanclinlele thenp w\n",
            "e,\n",
            "\n",
            "Maver:\n"
          ]
        }
      ]
    }
  ]
}